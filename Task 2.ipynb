{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42691,"status":"ok","timestamp":1675477880995,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"},"user_tz":0},"id":"m7-IbGKFu5FI","outputId":"b39a2fae-3d30-4c40-c9a6-ebf1a3a5dde1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import numpy as np\n","\n","path = \"/content/drive/MyDrive/ColabNotebooks/MachineLearningCoursework/\" # Change path to run program\n","train_x = np.load(path + \"train_x.npy\", allow_pickle=True)\n","train_y = np.load(path + \"train_y.npy\", allow_pickle=True)\n","test_x = np.load(path + \"test_x.npy\", allow_pickle=True)\n","test_y = np.load(path + \"test_y.npy\", allow_pickle=True)\n","valid_x = np.load(path + \"valid_x.npy\", allow_pickle=True)\n","valid_y = np.load(path + \"valid_y.npy\", allow_pickle=True)"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"oS3Empqg6_I2","executionInfo":{"status":"ok","timestamp":1675477880996,"user_tz":0,"elapsed":8,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"}}},"outputs":[],"source":["# Code used from view.ipynb provided\n","import matplotlib.pyplot as plt\n","\n","def image_normalization(arr):\n","    return (arr - arr.min())/(arr.max()-arr.min())\n","\n","def disable_ax_ticks(ax):\n","    ax.set_xticks([])\n","    ax.set_xticks([], minor=True)\n","    ax.set_yticks([])\n","    ax.set_yticks([], minor=True)\n","\n","def show_mnist_examples(x, y):\n","    fig = plt.figure(constrained_layout=True,figsize=(12,9), dpi=100)\n","    gs = fig.add_gridspec(3,4)\n","    main_ax = fig.add_subplot(gs[:3,:3])\n","    fig.suptitle(y)\n","    main_ax.imshow(image_normalization(np.moveaxis(x, 0, -1)))\n","    disable_ax_ticks(main_ax)\n","\n","    for j in range(3):\n","      c_ax = fig.add_subplot(gs[j,-1])\n","      subimage = x.copy()\n","      subimage[:j] = 0\n","      subimage[j+1:] = 0\n","      subimage[j] = subimage[j]-subimage[j].min()\n","      c_ax.imshow(image_normalization(np.moveaxis(subimage, 0, -1)))\n","      disable_ax_ticks(c_ax)\n","    plt.show()"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":658},"executionInfo":{"elapsed":1607,"status":"ok","timestamp":1675477882596,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"},"user_tz":0},"id":"vgLGRhoXOmi1","outputId":"5e6360ab-5ec7-4695-f4ef-69af305a6c2b"},"outputs":[{"output_type":"display_data","data":{"text/plain":["<Figure size 1200x900 with 4 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import random\n","\n","# Display random image from training dataset\n","ri = random.randrange(train_x.shape[0])\n","show_mnist_examples(train_x[ri], train_y[ri])\n","\n","# Reshape train, test and validate datasets to be channel last for the model\n","train_x = train_x.reshape(train_x.shape[0], 28, 28, 3)\n","test_x = test_x.reshape(test_x.shape[0], 28, 28, 3)\n","valid_x = valid_x.reshape(valid_x.shape[0], 28, 28, 3)\n","\n","train_x = image_normalization(train_x)\n","test_x = image_normalization(test_x)\n","valid_x = image_normalization(valid_x)"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":9314,"status":"ok","timestamp":1675477891906,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"},"user_tz":0},"id":"O83lshVOpV8T","colab":{"base_uri":"https://localhost:8080/"},"outputId":"063bdee2-fae4-4696-87a5-a3f54d644c9b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5\n","58889256/58889256 [==============================] - 3s 0us/step\n"]}],"source":["from tensorflow.keras.applications import VGG16\n","\n","# Set VGG16 as the base model with include_top=False to disclude the top layers of the model\n","base_model = VGG16(weights=\"imagenet\", input_shape=(224, 224, 3), include_top=False)\n","# Freeze the weights of the base model\n","base_model.trainable = False"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":24,"status":"ok","timestamp":1675477891907,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"},"user_tz":0},"id":"UJIC-oJrwv55","outputId":"343ef398-b805-463a-d146-23a9b99ecbd7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"vgg16\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n","                                                                 \n"," block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n","                                                                 \n"," block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n","                                                                 \n"," block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n","                                                                 \n"," block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n","                                                                 \n"," block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n","                                                                 \n"," block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n","                                                                 \n"," block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n","                                                                 \n"," block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n","                                                                 \n"," block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n","                                                                 \n"," block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n","                                                                 \n"," block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n","                                                                 \n"," block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n","                                                                 \n"," block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n","                                                                 \n"," block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n","                                                                 \n","=================================================================\n","Total params: 14,714,688\n","Trainable params: 0\n","Non-trainable params: 14,714,688\n","_________________________________________________________________\n"]}],"source":["base_model.summary()"]},{"cell_type":"markdown","source":["This model took the largest amount of time to get right, a lot of research into which layers should be used was conducted. Many examples found used very aggressive dropout values, but it was found that a lower value worked better for this model through testing. Different layouts for the dense layers were tested until layers that worked well with the pre-trained model were found. Batch normalisation layers between each of the dense layers helped with this as well."],"metadata":{"id":"EuC5hzJMpMb4"}},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":673,"status":"ok","timestamp":1675477892566,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"},"user_tz":0},"id":"Cs9tXMYjGCa1","outputId":"6b1ab8cc-2bc2-497b-f091-af84116248b5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," lambda (Lambda)             (None, 224, 224, 3)       0         \n","                                                                 \n"," vgg16 (Functional)          (None, 7, 7, 512)         14714688  \n","                                                                 \n"," flatten (Flatten)           (None, 25088)             0         \n","                                                                 \n"," batch_normalization (BatchN  (None, 25088)            100352    \n"," ormalization)                                                   \n","                                                                 \n"," dense (Dense)               (None, 1024)              25691136  \n","                                                                 \n"," batch_normalization_1 (Batc  (None, 1024)             4096      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_1 (Dense)             (None, 512)               524800    \n","                                                                 \n"," batch_normalization_2 (Batc  (None, 512)              2048      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_2 (Dense)             (None, 256)               131328    \n","                                                                 \n"," batch_normalization_3 (Batc  (None, 256)              1024      \n"," hNormalization)                                                 \n","                                                                 \n"," dense_3 (Dense)             (None, 128)               32896     \n","                                                                 \n"," batch_normalization_4 (Batc  (None, 128)              512       \n"," hNormalization)                                                 \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout (Dropout)           (None, 64)                0         \n","                                                                 \n"," batch_normalization_5 (Batc  (None, 64)               256       \n"," hNormalization)                                                 \n","                                                                 \n"," dense_5 (Dense)             (None, 20)                1300      \n","                                                                 \n","=================================================================\n","Total params: 41,212,692\n","Trainable params: 26,443,860\n","Non-trainable params: 14,768,832\n","_________________________________________________________________\n"]}],"source":["from tensorflow.keras import layers, models\n","import tensorflow as tf\n","\n","# Resolution to reshape images to\n","new_res = (224, 224)\n","\n","# Defining the model with the base_model included\n","model = models.Sequential([\n","  layers.InputLayer(train_x[0].shape),\n","  # Resize images to fit model\n","  layers.Lambda(lambda image: tf.image.resize(image, new_res)),\n","  base_model,\n","  # Flatten the output of the base model into one dimension\n","  layers.Flatten(),\n","  layers.BatchNormalization(),\n","  # First dense layer\n","  layers.Dense(1024, activation='relu'),\n","  layers.BatchNormalization(),\n","  # Second dense layer\n","  layers.Dense(512, activation='relu'),\n","  layers.BatchNormalization(),\n","  # Third dense layer\n","  layers.Dense(256, activation='relu'),\n","  layers.BatchNormalization(),\n","  # Fourth dense layer\n","  layers.Dense(128, activation='relu'),\n","  layers.BatchNormalization(),\n","  # Fifth dense layer\n","  layers.Dense(64, activation='relu'),\n","  # Dropout layer to reduce overfitting\n","  layers.Dropout(0.1),\n","  layers.BatchNormalization(),\n","  # Final dense layer for prediction\n","  layers.Dense(20, activation='softmax')                  \n","])\n","\n","model.summary()"]},{"cell_type":"markdown","source":["50 epochs was chosen to stop the model from training for too long if early stopping hadn't stopped it by 50 epochs. Different learning rates were tested for this model, but the default learning rate of 0.001 for Adam ended up giving the best results."],"metadata":{"id":"6ZUHmcacrRT9"}},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n2KZl7rwGGMg","executionInfo":{"status":"ok","timestamp":1675485395346,"user_tz":0,"elapsed":7486289,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"}},"outputId":"67d6c947-598e-4127-aba0-0db6c6da1578"},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/50\n","1407/1407 [==============================] - 301s 213ms/step - loss: 2.1496 - accuracy: 0.2684 - val_loss: 1.6938 - val_accuracy: 0.3910\n","Epoch 2/50\n","1407/1407 [==============================] - 298s 212ms/step - loss: 1.6093 - accuracy: 0.4427 - val_loss: 1.3030 - val_accuracy: 0.5626\n","Epoch 3/50\n","1407/1407 [==============================] - 298s 212ms/step - loss: 1.1970 - accuracy: 0.6025 - val_loss: 0.9542 - val_accuracy: 0.6950\n","Epoch 4/50\n","1407/1407 [==============================] - 298s 212ms/step - loss: 0.8516 - accuracy: 0.7254 - val_loss: 0.7727 - val_accuracy: 0.7678\n","Epoch 5/50\n","1407/1407 [==============================] - 298s 212ms/step - loss: 0.6087 - accuracy: 0.8090 - val_loss: 0.7032 - val_accuracy: 0.7939\n","Epoch 6/50\n","1407/1407 [==============================] - 298s 212ms/step - loss: 0.4679 - accuracy: 0.8523 - val_loss: 0.6727 - val_accuracy: 0.8087\n","Epoch 7/50\n","1407/1407 [==============================] - 298s 211ms/step - loss: 0.3725 - accuracy: 0.8820 - val_loss: 0.6726 - val_accuracy: 0.8209\n","Epoch 8/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.3052 - accuracy: 0.9044 - val_loss: 0.6323 - val_accuracy: 0.8347\n","Epoch 9/50\n","1407/1407 [==============================] - 308s 219ms/step - loss: 0.2494 - accuracy: 0.9219 - val_loss: 0.6530 - val_accuracy: 0.8385\n","Epoch 10/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.2255 - accuracy: 0.9295 - val_loss: 0.6705 - val_accuracy: 0.8374\n","Epoch 11/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1962 - accuracy: 0.9389 - val_loss: 0.6561 - val_accuracy: 0.8422\n","Epoch 12/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1782 - accuracy: 0.9450 - val_loss: 0.6814 - val_accuracy: 0.8420\n","Epoch 13/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1584 - accuracy: 0.9508 - val_loss: 0.6503 - val_accuracy: 0.8454\n","Epoch 14/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1414 - accuracy: 0.9559 - val_loss: 0.6630 - val_accuracy: 0.8478\n","Epoch 15/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1366 - accuracy: 0.9587 - val_loss: 0.6353 - val_accuracy: 0.8539\n","Epoch 16/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1240 - accuracy: 0.9614 - val_loss: 0.7052 - val_accuracy: 0.8453\n","Epoch 17/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1193 - accuracy: 0.9624 - val_loss: 0.6797 - val_accuracy: 0.8530\n","Epoch 18/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1112 - accuracy: 0.9657 - val_loss: 0.6617 - val_accuracy: 0.8579\n","Epoch 19/50\n","1407/1407 [==============================] - 297s 211ms/step - loss: 0.1056 - accuracy: 0.9678 - val_loss: 0.6711 - val_accuracy: 0.8566\n","Epoch 20/50\n","1407/1407 [==============================] - 296s 210ms/step - loss: 0.0978 - accuracy: 0.9701 - val_loss: 0.6647 - val_accuracy: 0.8590\n","Epoch 21/50\n","1407/1407 [==============================] - 296s 210ms/step - loss: 0.0947 - accuracy: 0.9708 - val_loss: 0.6730 - val_accuracy: 0.8594\n","Epoch 22/50\n","1407/1407 [==============================] - 296s 210ms/step - loss: 0.0855 - accuracy: 0.9741 - val_loss: 0.6769 - val_accuracy: 0.8658\n","Epoch 23/50\n","1407/1407 [==============================] - 295s 210ms/step - loss: 0.0885 - accuracy: 0.9732 - val_loss: 0.6805 - val_accuracy: 0.8605\n","Epoch 24/50\n","1407/1407 [==============================] - 295s 210ms/step - loss: 0.0776 - accuracy: 0.9762 - val_loss: 0.6910 - val_accuracy: 0.8611\n","Epoch 25/50\n","1407/1407 [==============================] - 296s 210ms/step - loss: 0.0781 - accuracy: 0.9770 - val_loss: 0.6651 - val_accuracy: 0.8649\n","313/313 [==============================] - 49s 157ms/step - loss: 0.6835 - accuracy: 0.8595\n"]},{"output_type":"execute_result","data":{"text/plain":["[0.6834878921508789, 0.859499990940094]"]},"metadata":{},"execution_count":8}],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Compile, fit and evaluate model\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy'],\n",")\n","\n","# Stops model fitting if the accuracy of 3 epochs in a row is below the maximum\n","# accuracy achieved on a previous epoch\n","es = EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n","\n","model_output = model.fit(x=train_x, y=train_y, epochs=50, validation_data=(valid_x, valid_y), callbacks=[es])\n","model.evaluate(test_x, test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qq2OIEQyhCDP","executionInfo":{"status":"aborted","timestamp":1675477906936,"user_tz":0,"elapsed":11,"user":{"displayName":"Christopher Harvey","userId":"03665955858934014968"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","acc = model_output.history['accuracy']\n","val_acc = model_output.history['val_accuracy']\n","loss = model_output.history['loss']\n","val_loss = model_output.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","# Displays a plot of the training accuracy and validation accuracy over each epoch\n","plt.plot(epochs, acc, 'bo--', label='Training acc')\n","plt.plot(epochs, val_acc, 'ro--', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","# Displays a plot of the training loss and validation loss over each epoch\n","plt.plot(epochs, loss, 'bo--', label='Training loss')\n","plt.plot(epochs, val_loss, 'ro--', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyPdM/eHE1yLo7hTgQaUQkrM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}