{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22936,"status":"ok","timestamp":1641918918989,"user":{"displayName":"Christopher Harvey","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjnOVMhE7IDWvOZIbAwU3Uq-k9yViO6GZNeY01N7Ip5od-S3d9LX57UqU49DbepcWwQ7EKyu1GYWbBpsS8ynzrQ4krT7Y_ZbSWWJRMvS87qEjId6LYL5-5Jh21GlIqzMcoHk5PllOcuI7fSYFr6XDL4MjwYQDBjHvJWBiWt1ByK6ljCc8mzD3aR0xKHRF6Ievm1ljjzZynFoi2dTQCg8zr4JOzoLg7S6hmij-0_Rxrgv4YOKATilRhkAYMxUlSY3ZekZcD9V5_yhgI-0cA8NjrAY0NEFsgvttP1xJZJpqDUw9-9c8GsLaIDGW0rz3npb97_UHO3FvXSK9vQdLWpylCCsls-6nzdHCURMF5cGOil13qWouI_yevzNg_DWN5QbcD40Rp2fx_eX1M6GYYwMoMCrZD8rPB3VXbL-mW8Pjzc1zLEPxLy3tJlXFmwKQP-hSpVPv2qOPNDFneq3L33LXVS8Rhw3EMlCxeFHcdoJoei8_1WQvrkKVEcDUfRI5-eyalXwDze1ab68YbSswD9MEws-c9pCVydr2krsB-a--csS64Dj_XGMWx2XcrbgWpCA0l-envEBTVDNO8bam6-9x4tHsp0fL2Q5QBTYn4sniKtwFtEk-7qN64dHjE22L4phb2dWJ4rjooUXbqOc3QLFsYBBNEjV59iA8sLDhTzBQmsdBlE_taQK7swVqWbmZ7gqxLpcRCTQE6wvA05D_Y5_iYMlBsamEVhLBOwb_YbCY1kwUITYHSotQ-uwx10hD16uHMffg=s64","userId":"03665955858934014968"},"user_tz":0},"id":"YG0a5-nDgAmy","outputId":"c3e2f8b9-da99-45bf-d15c-47dd4bf53824"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import numpy as np\n","\n","path = \"/content/drive/MyDrive/ColabNotebooks/MachineLearningCoursework/\" # Change path to run program\n","train_x = np.load(path + \"train_x.npy\", allow_pickle=True)\n","train_y = np.load(path + \"train_y.npy\", allow_pickle=True)\n","test_x = np.load(path + \"test_x.npy\", allow_pickle=True)\n","test_y = np.load(path + \"test_y.npy\", allow_pickle=True)\n","valid_x = np.load(path + \"valid_x.npy\", allow_pickle=True)\n","valid_y = np.load(path + \"valid_y.npy\", allow_pickle=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M7ab5c4ZgsBR"},"outputs":[],"source":["# Code used from view.ipynb provided\n","import matplotlib.pyplot as plt\n","\n","def image_normalization(arr):\n","    return (arr - arr.min())/(arr.max()-arr.min())\n","\n","def disable_ax_ticks(ax):\n","    ax.set_xticks([])\n","    ax.set_xticks([], minor=True)\n","    ax.set_yticks([])\n","    ax.set_yticks([], minor=True)\n","\n","def show_mnist_examples(x, y):\n","    fig = plt.figure(constrained_layout=True,figsize=(12,9), dpi=100)\n","    gs = fig.add_gridspec(3,4)\n","    main_ax = fig.add_subplot(gs[:3,:3])\n","    fig.suptitle(y)\n","    main_ax.imshow(image_normalization(np.moveaxis(x, 0, -1)))\n","    disable_ax_ticks(main_ax)\n","\n","    for j in range(3):\n","      c_ax = fig.add_subplot(gs[j,-1])\n","      subimage = x.copy()\n","      subimage[:j] = 0\n","      subimage[j+1:] = 0\n","      subimage[j] = subimage[j]-subimage[j].min()\n","      c_ax.imshow(image_normalization(np.moveaxis(subimage, 0, -1)))\n","      disable_ax_ticks(c_ax)\n","    plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"T1RiJFzagyJb"},"outputs":[],"source":["import random\n","\n","# Display random image from training dataset\n","ri = random.randrange(train_x.shape[0])\n","show_mnist_examples(train_x[ri], train_y[ri])\n","\n","# Reshape train, test and validate datasets to be channel last for the model\n","train_x = train_x.reshape(train_x.shape[0], 28, 28, 3)\n","test_x = test_x.reshape(test_x.shape[0], 28, 28, 3)\n","valid_x = valid_x.reshape(valid_x.shape[0], 28, 28, 3)\n","\n","train_x = image_normalization(train_x)\n","test_x = image_normalization(test_x)\n","valid_x = image_normalization(valid_x)"]},{"cell_type":"markdown","source":["Initially for the model for this task we studied the layers of the VGG16 model used for task 2, this was to understand the layers in the model and what they were doing. The layers used for VGG16 with the extra layers added for task 2 were tried for this task. However the train times per epoch for this model were far too long and the accuracies achieved were not sufficient. This provided some insight into what the layers in VGG16 were actually doing, but that solution was not going to work. After this research was conducted into custom-built machine learning models for images of size 28x28 which were much more useful. This provided more insight into what convolutional layers to use and the different types of pooling layers that could be used."],"metadata":{"id":"-4q0MEhSrqa4"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"db1jQux4g_un"},"outputs":[],"source":["from tensorflow.keras import layers, models\n","\n","model = models.Sequential([\n","  layers.InputLayer(train_x[0].shape),\n","  # First convolutional layer\n","  layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),\n","  layers.BatchNormalization(),\n","  # Second convolutional layer\n","  layers.Conv2D(filters=32, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),\n","  layers.BatchNormalization(),\n","  # Pooling layer to downsample input\n","  layers.MaxPool2D(pool_size=(4, 4)),\n","  # Third convolutional layer\n","  layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),\n","  layers.BatchNormalization(),\n","  # Fourth convolutional layer\n","  layers.Conv2D(filters=64, kernel_size=(3, 3), strides=(1, 1), padding='same', activation='relu'),\n","  layers.BatchNormalization(),\n","  # Pooling layer\n","  layers.GlobalAveragePooling2D(),\n","  # Dense layer for prediction\n","  layers.Dense(units=20, activation='softmax')\n","])\n","\n","model.summary()"]},{"cell_type":"markdown","source":["50 epochs was chosen to stop the model from training for too long if early stopping hadn't stopped it by 50 epochs. Different learning rates were tested for this model, but the default learning rate of 0.001 for Adam ended up giving the best results."],"metadata":{"id":"Hkt92w7lriJg"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"tfWhw-7ShDEm"},"outputs":[],"source":["from tensorflow.keras.callbacks import EarlyStopping\n","\n","# Compile, fit and evaluate model\n","model.compile(\n","    optimizer='adam',\n","    loss='sparse_categorical_crossentropy',\n","    metrics=['accuracy'],\n",")\n","\n","# Stops model fitting if the accuracy of 3 epochs in a row is below the maximum\n","# accuracy achieved on a previous epoch\n","es = EarlyStopping(monitor='val_accuracy', mode='max', patience=3, restore_best_weights=True)\n","\n","model_output = model.fit(x=train_x, y=train_y, epochs=50, validation_data=(valid_x, valid_y), callbacks=[es])\n","model.evaluate(test_x, test_y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eBh1lRVQhIhN"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","acc = model_output.history['accuracy']\n","val_acc = model_output.history['val_accuracy']\n","loss = model_output.history['loss']\n","val_loss = model_output.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","# Displays a plot of the training accuracy and validation accuracy over each epoch\n","plt.plot(epochs, acc, 'bo--', label='Training acc')\n","plt.plot(epochs, val_acc, 'ro--', label='Validation acc')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","\n","plt.figure()\n","\n","# Displays a plot of the training loss and validation loss over each epoch\n","plt.plot(epochs, loss, 'bo--', label='Training loss')\n","plt.plot(epochs, val_loss, 'ro--', label='Validation loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[],"authorship_tag":"ABX9TyP5UCBUnZL3kcY/59Asb7qy"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}